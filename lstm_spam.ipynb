{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eW2CxZ7SSBmk"
      },
      "outputs": [],
      "source": [
        "! wget https://dl.fbaipublicfiles.com/fasttext/vectors-wiki/wiki.en.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S4f0b19rTdAv"
      },
      "outputs": [],
      "source": [
        "! wget https://dl.fbaipublicfiles.com/fasttext/vectors-wiki/wiki.ar.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5yMHCNNcTryt"
      },
      "outputs": [],
      "source": [
        "! unzip wiki.en.zip "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QiqxM0luUaTB"
      },
      "outputs": [],
      "source": [
        "! unzip wiki.ar.zip "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2UjvO-NbAZ1Z"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional, Conv1D, MaxPooling1D\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from gensim.models import FastText\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YcnysaldAdsb"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Download NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SqSLkc86Af72"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Load the CSV file\n",
        "df = pd.read_csv('https://raw.githubusercontent.com/LokasWiki/public-datasets/main/Datasets/usernames_spam.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6gB2XqwRAiZf"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Convert the 'Category' column to numeric labels\n",
        "le = LabelEncoder()\n",
        "df['Category'] = le.fit_transform(df['Category'])\n",
        "\n",
        "# Lowercase the 'Name' column\n",
        "df['Name'] = df['Name'].apply(lambda x: x.lower())\n",
        "\n",
        "# Combine the stop words for English and Arabic\n",
        "stop_words = set(stopwords.words('english')).union(set(stopwords.words('arabic')))\n",
        "\n",
        "# Remove stop words from the 'Name' column\n",
        "df['Name'] = df['Name'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))\n",
        "\n",
        "# Stem the 'Name' column\n",
        "stemmer = PorterStemmer()\n",
        "df['Name'] = df['Name'].apply(lambda x: ' '.join([stemmer.stem(word) for word in x.split()]))\n",
        "\n",
        "# Lemmatize the 'Name' column\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "df['Name'] = df['Name'].apply(lambda x: ' '.join([lemmatizer.lemmatize(word) for word in word_tokenize(x)]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MOTHKzH3AsP-"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Convert the numeric labels to one-hot encoding\n",
        "y = to_categorical(df['Category'])\n",
        "\n",
        "# Get the 'Name' column\n",
        "X = df['Name']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h5aNdrUJAthi"
      },
      "outputs": [],
      "source": [
        "# Tokenize the 'Name' column using Keras tokenizer\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(X)\n",
        "X = tokenizer.texts_to_sequences(X)\n",
        "\n",
        "# Pad the sequences to a fixed length\n",
        "max_length = 20\n",
        "X = pad_sequences(X, maxlen=max_length)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rFGGpty_Awy_"
      },
      "outputs": [],
      "source": [
        "\n",
        "#noto:  if have more ram add en wiki\n",
        "# Load the pre-trained word embeddings for Arabic\n",
        "arabic_word_vectors = FastText.load_fasttext_format('/content/wiki.ar.bin')\n",
        "\n",
        "# Create an embedding matrix for the words in the vocabulary\n",
        "embedding_dim = 300\n",
        "embedding_matrix = np.zeros((len(tokenizer.word_index) + 1, embedding_dim))\n",
        "for word, i in tokenizer.word_index.items():\n",
        "    if word in arabic_word_vectors.wv.vocab:\n",
        "        embedding_matrix[i] = arabic_word_vectors[word]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RFDNxhLtA6M_"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Define the Keras model\n",
        "model = Sequential()\n",
        "model.add(Embedding(len(tokenizer.word_index) + 1, embedding_dim, weights=[embedding_matrix], input_length=max_length, trainable=False))\n",
        "model.add(Conv1D(64, 5, activation='relu'))\n",
        "model.add(MaxPooling1D(pool_size=4))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Bidirectional(LSTM(64)))\n",
        "model.add(Dense(2, activation='softmax'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YCL4MMciA8U7"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Compile the model\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3M6EVcVUBADg"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Train the model\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
        "early_stopping = EarlyStopping(patience=3, restore_best_weights=True)\n",
        "history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10, batch_size=32, callbacks=[early_stopping])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bX_4p30kJ776"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
        "print('Test loss:', test_loss)\n",
        "print('Test accuracy:', test_acc)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1iBdSMj3SAt-"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gtVYKcjLkRgH"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bXfD7UB4krOl"
      },
      "outputs": [],
      "source": [
        "# Define the names to check\n",
        "new_names = [\"loka\", \"صف السادس\",\"fuck you\",\"محمد احمد عبد الغار علي\"]\n",
        "\n",
        "# Tokenize and pad the new names\n",
        "new_sequences = tokenizer.texts_to_sequences(new_names)\n",
        "new_sequences = pad_sequences(new_sequences, maxlen=20)\n",
        "\n",
        "# Make predictions on the new names\n",
        "predictions = loaded_model.predict(new_sequences)\n",
        "\n",
        "# Print the predictions\n",
        "for i, name in enumerate(new_names):\n",
        "    prediction = np.argmax(predictions[i])\n",
        "    if prediction == 0:\n",
        "        print(f\"{name} is not spam.\")\n",
        "    else:\n",
        "        print(f\"{name} is spam.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CikzfyUI5z_9"
      },
      "outputs": [],
      "source": [
        "# Save the trained model\n",
        "model.save('my_h5_model.h5')\n",
        "\n",
        "# Save the tokenizer\n",
        "import json\n",
        "tokenizer_json = tokenizer.to_json()\n",
        "with open('tokenizer.json', 'w', encoding='utf-8') as f:\n",
        "    f.write(json.dumps(tokenizer_json, ensure_ascii=False))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D3xbrtCe6kX2"
      },
      "outputs": [],
      "source": [
        "# Load the saved model\n",
        "from tensorflow.keras.models import load_model\n",
        "loaded_model = load_model('my_h5_model.h5')\n",
        "\n",
        "# Load the saved tokenizer\n",
        "with open('tokenizer.json', 'r', encoding='utf-8') as f:\n",
        "    tokenizer_json = json.loads(f.read())\n",
        "from tensorflow.keras.preprocessing.text import tokenizer_from_json\n",
        "loaded_tokenizer = tokenizer_from_json(tokenizer_json)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OfWGsh5Y6sZ9"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import tokenizer_from_json\n",
        "import json\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "class NameClassifier:\n",
        "    def __init__(self, model_path, tokenizer_path):\n",
        "        self.model = load_model(model_path)\n",
        "        with open(tokenizer_path, 'r') as f:\n",
        "            data = json.load(f)\n",
        "            self.tokenizer = tokenizer_from_json(data)\n",
        "\n",
        "        # Combine the stop words for English and Arabic\n",
        "        self.stop_words = set(stopwords.words('english')).union(set(stopwords.words('arabic')))\n",
        "\n",
        "        # Initialize stemmer and lemmatizer\n",
        "        self.stemmer = PorterStemmer()\n",
        "        self.lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "    def preprocess_name(self, name):\n",
        "        # Lowercase the name\n",
        "        name = name.lower()\n",
        "\n",
        "        # Remove stop words\n",
        "        name = ' '.join([word for word in name.split() if word not in self.stop_words])\n",
        "\n",
        "        # Stem the name\n",
        "        name = ' '.join([self.stemmer.stem(word) for word in name.split()])\n",
        "\n",
        "        # Lemmatize the name\n",
        "        name = ' '.join([self.lemmatizer.lemmatize(word) for word in word_tokenize(name)])\n",
        "\n",
        "        return name\n",
        "\n",
        "    def predict_category(self, name):\n",
        "        # Preprocess the name\n",
        "        name = self.preprocess_name(name)\n",
        "\n",
        "        # Tokenize the name\n",
        "        name_seq = self.tokenizer.texts_to_sequences([name])\n",
        "\n",
        "        # Pad the sequence to a fixed length\n",
        "        name_seq = pad_sequences(name_seq, maxlen=self.model.input_shape[1])\n",
        "\n",
        "        # Make the prediction\n",
        "        prediction = self.model.predict(name_seq)\n",
        "\n",
        "        # Return the predicted category\n",
        "        return np.argmax(prediction, axis=1)[0]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n_4TseDo60NM"
      },
      "outputs": [],
      "source": [
        "# Create an instance of the helper class\n",
        "name_classifier = NameClassifier('my_h5_model.h5', 'tokenizer.json')\n",
        "\n",
        "# Classify a name\n",
        "name = 'يسوع المسيح'\n",
        "category = name_classifier.predict_category(name)\n",
        "print('Name:', name)\n",
        "print('Category:', category)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lO5R3saU7smH"
      },
      "outputs": [],
      "source": [
        "# Load the CSV file\n",
        "df = pd.read_csv('/content/quarry-72406-untitled-run718464.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bwZk70Ng70Pm"
      },
      "outputs": [],
      "source": [
        "# Create an instance of the helper class\n",
        "name_classifier = NameClassifier('my_h5_model.h5', 'tokenizer.json')\n",
        "\n",
        "for index, row in df.iterrows():\n",
        "\n",
        "  # Classify a name\n",
        "  name = row[0]\n",
        "  category = name_classifier.predict_category(name)\n",
        "  print('Name:', name)\n",
        "  print('Category:', category)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QZ_afqg68Kqy"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
